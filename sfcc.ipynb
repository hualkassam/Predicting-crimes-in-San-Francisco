{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author__ Hussam Qassim__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime Classifier using BLSTM neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Import the main necessary libraries\n",
    "import os\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "def rset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Check TensorFlow Version. Please use TensorFlow version 1.0 or newer\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0')\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done..\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of features (x) and a vector of class labels (y)\n",
    "x = []\n",
    "y = []\n",
    "with open('dataset/train.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            date = re.search(\"([0-9]{4})-([0-9]{2})-([0-9]{2})\", row[0]).groups()\n",
    "            date = [int(x) for x in date]\n",
    "            time = re.search(\"([0-9]{2}):([0-9]{2}):([0-9]{2})\", row[0]).groups()\n",
    "            time = [int(x) for x in time]\n",
    "            category_string = row[1]\n",
    "            dayofweek_string = row[3]\n",
    "            pddistrict_string = row[4]\n",
    "            longitude = float(row[7])\n",
    "            latitude = float(row[8])\n",
    "            x_row = date + time + [longitude, latitude, \\\n",
    "                    dayofweek_string, pddistrict_string]\n",
    "            y_label = category_string\n",
    "            x.append(x_row)\n",
    "            y.append(y_label)\n",
    "\n",
    "# One-hot encoding for dayofweek and pddistrict vars\n",
    "dayofweek_set = set()\n",
    "pddistrict_set = set()\n",
    "for row in x:\n",
    "    dayofweek_set.add(row[-2])\n",
    "    pddistrict_set.add(row[-1])\n",
    "dayofweek_dict = {item: i for i, item in enumerate(dayofweek_set)}\n",
    "pddistrict_dict = {item: i for i, item in enumerate(pddistrict_set)}\n",
    "num_unique_dayofweek = len(dayofweek_dict)\n",
    "num_unique_pddistrict = len(pddistrict_dict)\n",
    "for i, row in enumerate(x):\n",
    "    encoded_dayofweek = [0]*num_unique_dayofweek\n",
    "    encoded_pddistrict = [0]*num_unique_pddistrict\n",
    "    current_dayofweek = row[-2]\n",
    "    current_pddistrict = row[-1]\n",
    "    encoded_dayofweek[dayofweek_dict[current_dayofweek]] = 1\n",
    "    encoded_pddistrict[pddistrict_dict[current_pddistrict]] = 1\n",
    "    x[i] = row[:-2] + encoded_dayofweek + encoded_pddistrict\n",
    "\n",
    "print(y)\n",
    "\n",
    "# label binarization\n",
    "category_set = set()\n",
    "for label in y:\n",
    "    category_set.add(label)\n",
    "category_dict = {item: i for i, item in enumerate(sorted(category_set))}\n",
    "num_unique_category = len(category_dict)\n",
    "for i, label in enumerate(y):\n",
    "    y[i] = category_dict[label]\n",
    "\n",
    "# Convert x, y to numpy array\n",
    "x = np.asarray(x, dtype=np.float32)\n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing and shaping the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "n_features = 25 \n",
    "seq_len = 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Training, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into 80% Training, 10% Validation, and 10% Testing  \n",
    "split_idx = int(len(x)*0.8)\n",
    "train_x, val_x = x[:split_idx], x[split_idx:]\n",
    "train_y, val_y = y[:split_idx], y[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network parameters\n",
    "lstm_size = 256\n",
    "batch_size = 250\n",
    "n_layers = 2\n",
    "drop_out = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Placeholders for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_features], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')   \n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction phase_ Build the TF graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN Cell and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence \n",
    "as-is and the second on a reversed copy of the input sequence. This can provide additional context to the \n",
    "network and result in faster and even fuller learning on the problem\n",
    "'''\n",
    "# Build one BLSTM and Fully_connected layer with Softmax classifier\n",
    "with graph.as_default():\n",
    "    \n",
    "# Using He initialization can significantly reduce the vanishing/exploding gradients problems   \n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# Create the graph of 2 hidden BLSTM layer and one output layer\n",
    "    with tf.name_scope(\"BLSTM\"):\n",
    "        def lstm_cell():\n",
    "            '''\n",
    "This LSTM variant with extra connections called peephole connections: the previous long-term state is added as\n",
    "an input to the controllers of the forget gate and the input gate, and the current long-term tate is added as\n",
    "input to the controller of the output gate\n",
    "'''\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units=lstm_size, initializer=he_init, use_peepholes=True)\n",
    "            '''\n",
    "            Applying dropout between the LSTM layers to prevent overfitting the training set.The following \n",
    "code applies dropout to the inputs of each layer in the LSTM, dropping each input with a 50% probability\n",
    "'''\n",
    "            cell_drop = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "            return cell_drop\n",
    "    \n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(n_layers)])\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(n_layers)])\n",
    "   \n",
    "        outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, \n",
    "                                                          inputs=inputs_, dtype=tf.float32, \n",
    "                                                          scope=\"BiLSTM\")\n",
    "        outputs = tf.concat(axis = 2, values = outputs)\n",
    "        last_output = outputs[:,-1,:]\n",
    "    \n",
    "        logits = tf.contrib.layers.fully_connected(last_output, 39, activation_fn=None, scope=\"logits\")\n",
    "        \n",
    "# Create the cost function     \n",
    "    with tf.name_scope(\"loss\"): \n",
    "\n",
    "# Computes the cross entropy, it is equivalent to applying the softmax activation function and then\n",
    "# computing the cross entropy, but it is more efficient, and it properly takes care of corner cases like logits \n",
    "# equal to 0\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "# Computes the mean of elements across dimensions of a tensor\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "                \n",
    "# Craete the optimizer \n",
    "    with tf.name_scope(\"train\"): \n",
    "# Applying clip the gradients technique to lessen the exploding gradient problem in the LSTM\n",
    "        threshold = 0.5\n",
    "# Using Adam as optimizer because it is combines the ideas of Momentum optimization and RMSProp\n",
    "# Adam is an adaptive learning rate algorithm, it requires less tuning of the learning rate hyperparameter η. \n",
    "# We can often use the default value η = 0.001\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                                     for grad, var in grads_and_vars]\n",
    "        training_op = optimizer.apply_gradients(capped_gvs)\n",
    "        \n",
    "# Evaluate the NN     \n",
    "    with tf.name_scope(\"eval\"):    \n",
    "        correct = tf.nn.in_top_k(logits, labels_, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\" )\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        \n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution phase_Execute the TF graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a name and file directory for TensorBoard\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "# Create a Saver node\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Create early stopping      \n",
    "early_stopping = 0 \n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # initialize all variables\n",
    "    \n",
    "# Creates a node in the graph that will evaluate the reduce_mean value and write it to a TensorBoard \n",
    "# compatible binary log string called a summary \n",
    "    loss_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Creates a FileWriter that you will use to write summaries to logfiles in the log directory\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    print('training ..')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            x = x[:,np.newaxis,:]\n",
    "            #x = x.reshape(-1, seq_len, n_features)\n",
    "# Update the execution phase to evaluate the loss_summary node regularly during training(every 10 mini-batches)\n",
    "            if ii % 10 == 0:\n",
    "                summary_str = loss_summary.eval(feed_dict={inputs_: x, labels_: y, keep_prob: drop_out})\n",
    "                step = epochs * batch_size + ii\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={inputs_: x, labels_: y, keep_prob: drop_out})\n",
    "        acc_train = accuracy.eval(feed_dict={inputs_: x, labels_: y, keep_prob: drop_out})\n",
    "        for x, y in get_batches(val_x, val_y, batch_size):\n",
    "            x = x[:,np.newaxis,:]\n",
    "            #x = x.reshape(-1, seq_len, n_features)\n",
    "            acc_val = accuracy.eval(feed_dict={inputs_: x, labels_: y, keep_prob: 1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val)\n",
    "        if acc_val >= early_stopping:\n",
    "            # Save the best trained model\n",
    "            saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "            early_stopping = acc_val\n",
    "                        \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    print('Loading the saved checkpoint..')\n",
    "# Load the saved model \n",
    "    saver.restore(sess, \"checkpoints/sentiment.ckpt\")\n",
    "    print('Testing..')\n",
    "# Baching the test dataset\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        x = x[:,np.newaxis,:]\n",
    "        #x = x.reshape(-1, seq_len, n_features)\n",
    "        acc_test = accuracy.eval(feed_dict={inputs_: x, labels_: y, keep_prob: 1})\n",
    "    print(\"Test accuracy:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
